# Ollama base URL (pointing to local serve on port 7860)
OLLAMA_BASE_URL=http://localhost:7860

# LLM for generation
GENERATION_MODEL=gpt-oss:20b

# Embedding model for retrieval (Ollama)
EMBEDDING_MODEL=bge-m3:latest

# Chroma persistence directory and collection name
CHROMA_DIR=./data/chroma
CHROMA_COLLECTION=dmom_collection

# Defaults for chunking and retrieval
CHUNK_SIZE=800
CHUNK_OVERLAP=120
TOP_K=5

# Gemini (optional)
# If you want to use Gemini via REST instead of Ollama, set your API key and model.
# Default base URL targets the public v1 endpoint; override if needed.
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1

# Choose chat backend for generation: 'ollama' (default) or 'gemini'
CHAT_BACKEND=ollama
